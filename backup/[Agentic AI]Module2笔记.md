“反思模式”是经常使用且实现起来出奇的简单的一种方法。人类比如写邮件时也不是一次到位，而是写出草稿后进行检查反思。

### 一、技巧（干货！）：

> 1.明确指示反思动作 (Clearly indicate the reflection action):
> 不要含糊地说“请改进”，而要说“请审查”、“请检查”、“请验证”；明确告诉模型你要它做什么，例如“审核电子邮件初稿”或“验证HTML代码”
> 2.不同的 LLM 有不同的专长。可以使用一个擅长快速生成的模型来写初稿，再用一个“思考模型”（Thinking Model）、更擅长逻辑推理和错误排查的模型来进行反思和改进。**实践中一般推理模型或思考模型更善于reflection的工作。**
> 3.利用**外部反馈**帮助反思。引入来自LLM之外的新信息，对reflection常常有巨大的帮助。当你能使用外部信息时，尽可能地加入外部信息。
> 比如，生成代码的工作流中，加入运行代码的步骤，利用代码的实际输出（Output）和错误信息（Errors）输入到LLM中进行reflection。
> 4.最好给出**检查的具体要求或标准**(Specify criteria to check)。比如代码的语法，或生成的域名在英语中有没有负面含义、好不好发音等。
> 5.关于提示词的书写，可以参考别人写的觉得好的开源的一些提示词。


与反思相对应的称为“直接生成”（direct generation）或“零样本提示”（Zero-shot Prompting）（零样本是指没有给LLM提供期望输出的例子）。


### 二、评估（evals）
在比较直接生成还是添加反思、比较调试不同提示词时，我们需要一个系统化的评估来科学量化指导。
1.客观评估
比如，创建一个包含“提示词”和“真实答案”（ground truth answer）的数据集。客观评估的指标比较容易量化
2.主观评估
通常可以采用一个LLM进行评估。直接让其进行比较往往输出不稳定和一致，甚至有位置偏见（比如更倾向选择前一个）。
**使用评分表（rubric）**：更好的方法是为 LLM 提供一套结构化的评分标准（Rubric），让它对每个维度进行打分，而不是直接比较。比如给出十个标准，每个标准满足与否对应0或1，最后将十个累加起来得到1-10的打分。这比直接让LLM打分可靠的多。
对更复杂的主观标准（如图表美观度）可能需要更多调优。比如更改更精确的提示词等。



### 三、外部反馈

<img width="800" height="400" alt="Image" src="https://github.com/user-attachments/assets/48398a49-cb3b-4307-9505-89ea067ad596" />
再次强调使用外部反馈的作用。

some example:

<img width="800" height="380" alt="Image" src="https://github.com/user-attachments/assets/09a9de16-1bba-415e-8bc6-64537553d5de" />

其实我们人的工作流也是这样，不能自己干想，去做点实验，做一些具体的尝试，或者说“离问题更近一点”！